{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IRIS FLOWER CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "submitted by : **Abhinaya A.S.**\n",
    "\n",
    "This project was done as a part of the 1-month (10 Aug - 10 Sept, 2024), work from home internship offered to me by **CognoRise InfoTech Pvt. Ltd**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OVERVIEW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Knowing the Dataset**\n",
    "* multivariate data set \n",
    "* introduced by the British statistician and biologist **Ronald Fisher** in his 1936 paper **The use of multiple measurements in taxonomic problems**\n",
    "* It is sometimes called **Anderson's Iris data set**, because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species\n",
    "* consists of 50 samples from each of the 3 species of Iris (Iris Setosa, Iris virginica, and Iris versicolor)\n",
    "* 4 features were measured from each sample: the length, width of the sepals and petals, in cm.\n",
    "* No. of rows = 150, No. of columns = 5\n",
    "* format = csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Statement**\n",
    "To create a machine learning model which will catogerize an iris flower into its actual species based on the flower features like sepal length, sepal width, petal length and petal width. The performance of the model shall be evaluated using relevant criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Solving Approach**\n",
    "1. Learn the theory behind **Decision Tree Classification** algorithm\n",
    "2. Construct a classifier class using the OOP method from scratch, that works by using the decision tree classifier algorithm. Python programming language is used.\n",
    "3. Use the created class for building a decision tree classifier model\n",
    "4. Use a portion of the iris dataset to train the classification model\n",
    "5. Test the performance of the model on another portion of iris dataset and evaluate using appropriate performance metrics\n",
    "6. Discuss the implications of the performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SOLUTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 1 : Importing Required Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 2 : Defining the Decision tree based Classifier Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the class for a node in our decision tree\n",
    "class Node():\n",
    "    def __init__(self, factor_idx = None, cut_off = None, first = None, second = None, impurity_loss = None, type = None ):\n",
    "        # for decision node\n",
    "        self.factor_idx = factor_idx # index of the column in which the deciding factor viz. a feature is present\n",
    "        self.cut_off = cut_off # the value of the deciding feature upon which the splitting is done\n",
    "        self.first = first # first child of the node. any data point with value of deciding factor less than or equal to cut off value will be sent to this child\n",
    "        self.second = second # second child of the node\n",
    "        self.impurity_loss = impurity_loss # the loss in impurity of the data while moving from the node to it's children\n",
    "\n",
    "        # for leaf node\n",
    "        self.type = type # the catogery/type/class in which majority of the datapoints in the node belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the class for the decision tree which does the job of classification\n",
    "class DecisionTree_Catogerizer():\n",
    "    def __init__(self, min_data = 2, max_depth = 2):\n",
    "\n",
    "        self.root = None\n",
    "        self.min_data = min_data\n",
    "        self.max_depth = max_depth\n",
    "        # initialize the root node of the tree\n",
    "        \n",
    "\n",
    "    # defining the class methods \n",
    "\n",
    "    '''METHOD 1 - entropy\n",
    "    used for calculating the entropy in the data within a node'''\n",
    "    def entropy(self, y):\n",
    "\n",
    "        '''y represents the 2D column matrix (a numpy array). \n",
    "        It is basically the column of the target variable of the data inside a node.\n",
    "         No. of rows in this array is same as the number of datapoints present in the node. '''\n",
    "        \n",
    "        types = np.unique(y) # a list of catogeries to which the data points in the node belong\n",
    "        entropy = 0\n",
    "        # iterate over each catogery\n",
    "        for cls in types:\n",
    "            cls_proportion = len(y[ y==cls ])/len(y)\n",
    "            entropy += -cls_proportion* np.log2(cls_proportion)\n",
    "        return entropy\n",
    "    \n",
    "    '''METHOD 2 - gini_index\n",
    "    used for calculating the ginin index of data within a node. Method similar to entropy'''\n",
    "    def gini_index(self, y):\n",
    "        types = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in types:\n",
    "            cls_proportion = len(y[ y==cls ])/len(y)\n",
    "            gini += cls_proportion**2\n",
    "        return 1-gini\n",
    "    \n",
    "    '''METHOD 3 - cleave\n",
    "    used to divide the data in a node into it's child nodes based on the cut-off value of the deciding factor'''\n",
    "    def cleave(self, data, factor_idx, cut_off):\n",
    "\n",
    "        '''data : a 2D numpy array corresponding to the subset of the dataset\n",
    "          present in the node which is to be split'''\n",
    "        data_first = np.array([row for row in data if row[ factor_idx ] <= cut_off])\n",
    "        data_second = np.array([row for row in data if row[ factor_idx ] > cut_off])\n",
    "        \n",
    "        return data_first, data_second\n",
    "\n",
    "    '''METHOD 4 - info_gain\n",
    "    used for calculating the information gain value associated with a specific spliting of a node.\n",
    "    Each split is characterised by a specific deciding factor and its specific cut-off value'''\n",
    "    def info_gain(self, parent, f_child, s_child, mode = 'gini'):\n",
    "\n",
    "        '''parent  : represents the target variable column of the data in the node which was split\n",
    "           f_child : represents the target variable column of the data in the first_child node obtained after split\n",
    "           s_child : represents the target variable column of the data in the second_child node obtained after the split'''\n",
    "        # assigning weights to the child nodes\n",
    "        w_f = len(f_child)/len(parent)\n",
    "        w_s = len(s_child)/len(parent)\n",
    "\n",
    "        if mode == 'gini':\n",
    "            # calculating the weighted average of impurity in the child nodes\n",
    "            weighted_avg_impurity = w_f*self.gini_index(f_child) + w_s*self.gini_index(s_child)\n",
    "\n",
    "            # subtracting this from impurity of the parent node\n",
    "            gain = self.gini_index(parent) - weighted_avg_impurity\n",
    "            return gain\n",
    "        else :\n",
    "            weighted_avg_impurity = w_f*self.entropy(f_child) + w_s*self.entropy(s_child)\n",
    "            gain = self.entropy(parent) - weighted_avg_impurity\n",
    "            return gain\n",
    "        \n",
    "    '''METHOD 5 - perfect_cleave\n",
    "    returns a dictionary of information regarding the best possible cleavage of a node in the decision tree'''\n",
    "    def perfect_cleave(self, data, num_factors):\n",
    "\n",
    "        '''data : a 2D numpy array corresponding to a subset of the dataset predsent in a node.\n",
    "                  It's last column consists of the target variable values. Rest of the columns\n",
    "                  corresponds to various features    '''\n",
    "\n",
    "        # initializes an empty dictionary\n",
    "        perfect_cleave = {}\n",
    "        # sets value for maximum information gain which will be updated later\n",
    "        max_info_gain = -float('inf')\n",
    "\n",
    "        # outer loop to iterate over possible deciding factors\n",
    "        for idx in range(num_factors):\n",
    "            factor_values = data[:,idx]\n",
    "            cut_off_values = np.unique(factor_values)\n",
    "\n",
    "            # inner loop to iterate over possible cut_off values of the current deciding factor\n",
    "            for cut_off in cut_off_values:\n",
    "                # splitting the data in the node based on the current factor and current threshold\n",
    "                data_f, data_s = self.cleave(data, idx, cut_off)\n",
    "                if len(data_f)>0 and len(data_s)>0:\n",
    "                    parent_y = data[:,-1]\n",
    "                    f_child_y = data_f[:,-1]\n",
    "                    s_child_y = data_s[:,-1]\n",
    "                    current_info_gain = self.info_gain(parent_y, f_child_y, s_child_y)\n",
    "                    # checking if the information gain of the current split is more than that of previous split\n",
    "                    if current_info_gain > max_info_gain:\n",
    "                        # update the dictionary values\n",
    "                        perfect_cleave['factor_idx'] = idx\n",
    "                        perfect_cleave['cut_off'] = cut_off\n",
    "                        perfect_cleave['data_f'] = data_f\n",
    "                        perfect_cleave['data_s'] = data_s\n",
    "                        perfect_cleave['impurity_loss'] = current_info_gain\n",
    "                        max_info_gain = current_info_gain\n",
    "\n",
    "        return perfect_cleave\n",
    "    \n",
    "    '''METHOD 6 - find_leaf_type\n",
    "    to determine the catogery represented by a leaf node in the decision tree'''\n",
    "    def find_leaf_type(self, y):\n",
    "        y = list(y)\n",
    "        return max(y, key = y.count)\n",
    "\n",
    "    \n",
    "    '''METHOD 7 - make_tree\n",
    "    for building the decision tree by making nodes, splitting nodes and so on.\n",
    "    This is a recursive function'''\n",
    "    def make_tree(self, data, depth = 0):\n",
    "\n",
    "        '''data : a 2D array. Its last column consists of target variable value.\n",
    "                  Rest of the columns correspond to various features in the dataset'''\n",
    "        \n",
    "        # seperating feature matrix and target column from the data\n",
    "        X, y = data[:,:-1], data[:,-1]\n",
    "        num_samples, num_factors = np.shape(X)\n",
    "\n",
    "        # checking for stopping conditions\n",
    "        if num_samples >= self.min_data and depth <= self.max_depth:\n",
    "            split_dict = self.perfect_cleave(data, num_factors)\n",
    "            if split_dict['impurity_loss']>0:\n",
    "                first_subtree = self.make_tree(split_dict['data_f'],depth = depth + 1)\n",
    "                second_subtree = self.make_tree(split_dict['data_s'], depth = depth + 1)\n",
    "                return Node(split_dict['factor_idx'], split_dict['cut_off'], first_subtree, second_subtree, split_dict['impurity_loss'])\n",
    "        \n",
    "        # create leaf node\n",
    "        leaf_type = self.find_leaf_type(y)\n",
    "        return Node(type = leaf_type)\n",
    "    \n",
    "    '''METHOD 8 - fit\n",
    "    the method for training the catogerizer model using the training dataset'''\n",
    "    def fit(self, X, y):\n",
    "        '''X : represents the feature matrix of training dataset.\n",
    "               It is a 2D numpy array.\n",
    "           y : represents the column matrix of target variable in the training set.\n",
    "               It is a 2D numpy array'''\n",
    "        \n",
    "        # joining X,y to get complete dataset\n",
    "        dataset = np.concatenate((X,y), axis = 1)\n",
    "        # setting up root node of the tree by feeding entire training set into make_tree function\n",
    "        self.root = self.make_tree(dataset)\n",
    "    \n",
    "    \n",
    "\n",
    "    '''METHOD 9 - find_type\n",
    "    used for predicting the catogery of a given feature vector'''\n",
    "    \n",
    "    def find_type(self, x, tree):\n",
    "        if tree.type != None:\n",
    "            return tree.type\n",
    "        factor_value = x[tree.factor_idx]\n",
    "        if factor_value <= tree.cut_off:\n",
    "            return self.find_type(x, tree.first)\n",
    "        else:\n",
    "            return self.find_type(x, tree.second)\n",
    "\n",
    "    '''METHOD 10 - predict\n",
    "    used for predicting the types of data points for a given dataset'''\n",
    "    def predict(self, X):\n",
    "        predictions = [self.find_type(x, self.root) for x in X]\n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 3 : Loading the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the dataset to understand it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('IRIS.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    float64\n",
       "sepal_width     float64\n",
       "petal_length    float64\n",
       "petal_width     float64\n",
       "species          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 4 : Train-Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=392024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 5 : Initializing the Classification Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of 'DecisionTree_Catogerizer' class and storing it in the variable 'classify'\n",
    "classify = DecisionTree_Catogerizer(min_data = 3, max_depth = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 6 : Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the 'classify' model using the training dataset\n",
    "classify.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 7 : Testing the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model on the unseen test dataset and collecting the predictions made by the model\n",
    "y_pred = classify.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 8 : Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "* for a multi-class classification problem, with say n classes, it is an n by n matrix\n",
    "* It gives insights about classifications made by the model\n",
    "* diagonal elements give the correct predictions\n",
    "* off-diagonal elements give the incorrect predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "performance_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(performance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Values in confusion matrix** :\n",
    "\n",
    "to interpret values in the confusion matrix w.r.t any class in a multiclass problem, that class is regarded as positive and rest of the classes is regarded as negative. In the terms given below 'True/ False' indicate the correctness of classification done by the model. While the 'positive/ negative' indicate the class predicted by the model which may be correct or incorrect.\n",
    "* **True Positives (TP)** : for a particular class, this denotes the number of instances correctly classified by the model\n",
    "* **False Positives (FP)** : w.r.t a particular class, this denotes the number of instances that originally belong to other classes but incorrectly predicted by the model as positive\n",
    "* **False Negatives (FN)** : w.r.t a particular class, this denotes the number of instances in that class incorrectly classified by the model into other classes\n",
    "* **True Negatives (TN)** : w.r.t a particular class,this denotes the number of instances in other classes correctly predicted by the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy Score**\n",
    "\n",
    "* for any class in a multiclass problem, accuracy is determined by $\\frac{TP + TN}{TP + FP + FN + TN}$\n",
    "* accuracy of the model is determined by averaging accuracies over all classes\n",
    "* it is a value between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the 'classify' model is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the 'classify' model is {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high accuracy score in a classifier indicates that the model is making correct predictions most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "* for any class in a multiclass problem, accuracy is determined by $\\frac{TP}{TP + FP }$\n",
    "* precision of the model is determined by averaging precisions over all classes\n",
    "* it is a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison of the 'classify' model is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precison of the 'classify' model is {precision_score(y_test, y_pred, average='macro')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall Score**\n",
    "\n",
    "* for any class in a multiclass problem, accuracy is determined by $\\frac{TP}{TP + FN}$\n",
    "* recall of the model is determined by averaging recalls over all classes\n",
    "* it is a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of the 'classify' model is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recall of the 'classify' model is {recall_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 - Score**\n",
    "\n",
    "* for any class in a multiclass problem, F1-score is determined by $\\frac{2 * Precision * Recall}{Presion + Recall}$\n",
    "* F1-score of the model is determined by averaging F1-scores over all classes\n",
    "* it is a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score of the 'classify' model is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1-Score of the 'classify' model is {f1_score(y_test, y_pred, average ='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a success as it has highest value for all metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris_flower_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
